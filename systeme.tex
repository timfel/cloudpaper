\section{Live-Migration-Systeme}
\label{sec:live-migr-syst}
Im Folgenden wollen wir einige konkrete Implementierungen der oben
vorgestellten Techniken vorstellen, und deren Grenzen und
Anwendbarkeit für die oben genannten Anwendugsfälle betrachten.

\subsection{NomadBIOS und vMotion}
\label{sec:nomadbios--vmware}
Wie im Vortrag von Hansen beschrieben, war die erste vollständige
Implementierung zur Live-Migration einer kompletten \ac{VM} der
NomadBIOS-Hypervisor. Asger Jensen und Jacob Gorm Hansen entwickelten diesen
Hypervisor auf dem L4-Mikrokern und demonstrierten als erste
erfolgreich Live-Migration von virtuellen Maschinen über ein lokales
Netzwerk. Die Motivation dieser Arbeit kam aus dem Grid-Computing: Für
jeden Job sollte eine \ac{VM} gestartet werden, die dann, mit
Prioritäten ausgestattet, durch das Grid verschoben werden konnte, auf
der Suche nach freien Ressourcen. Daher rührt auch der Name
\emph{NomadBIOS}, ein "`nomadisches"' System.

Im Rahmen ihrer Masterarbeit implementierten Hansen und
Jensen den Hypervisor zunächst auf dem L4-Kern. Gastsysteme
mussten angepasst werden, um auf diesem teilvirtualisierten System als
Userspace-Prozess ausgeführt zu werden. Im Umkehrschluss konnten viele
der Prinzipien hinter Prozessmigration verwendet werden, um
Betriebssystemprozesse zu verschieben. Außerdem verwendet das
NomadBIOS \emph{Checkpointing}, den Akt, den Speicherzustand von
Prozessen einzufrieren um sie später fortsetzen zu können.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\linewidth]{images/nomad_stage1}
  \caption{NomadBIOS: 1. Migrationsphase}
  \label{fig:nomad_stage1}
\end{figure}


\subsubsection{Implementierungsdetails des NomadBIOS}
\label{sec:impl-des-nomadb}
Im NomadBIOS werden mehrere Userspace-Module für den L4-Kern
implementiert, die dazu dienen, Gastbetriebssysteme von der Hardware
zu isolieren~\cite{hansen2002nomadic}. Dazu gehören \emph{Paging}, für
die Isolation des Speichermanagements, \emph{Interrupt Multiplexing}
um Zugriff auf Hardware zu verweben und \emph{Address Space
  Translation}, um einfache Kommunikation zwischen dem Gast- und dem
Hostbetriebssystem zu ermöglichen. Letzteres wird genutzt, um
Unterstützung für den Migrationsvorgang im Gastsystem implementieren
zu können.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\linewidth]{images/nomad_stage2}
  \caption{NomadBIOS: 2. Migrationsphase}
  \label{fig:nomad_stage2}
\end{figure}
\begin{figure}[b]
  \centering
  \includegraphics[width=0.7\linewidth]{images/nomad_stage3}
  \caption{NomadBIOS: 3. Migrationsphase}
  \label{fig:nomad_stage3}
\end{figure}

Für die eigentliche Migration implementiert das NomadBIOS ein
\emph{Precopy}-Schema, wie es bereits aus der Prozessmigration bekannt
ist~\cite{hansen2002nomadic}.
\begin{enumerate}
\item In \autoref{fig:nomad_stage1} ist der Startzustand der
  Migration dargestellt. Initial ist der Zustand allen Speichers auf
  dem Zielsystem unbekannt (helles Grau). Der gesamte Speicher vom
  Ursprungssystem muss zum Kopieren ausgewählt werden (rot). Der
  Speicher auf dem Ursprung, der dem Gastsystem gehört, wird zunächst
  für Schreibzugriffe gesperrt und der Kopiervorgang auf den neuen
  Host wird gestartet. Wenn währenddessen Speicher geschrieben wird,
  wird ein Page-Fault ausgelöst, und das NomadBIOS kann die
  entsprechende Speicherseite erneut zum Kopieren markieren. Danach
  wird die Seite als für das Gastsystem beschreibbar markiert und
  geschrieben.
\item Nach einem ersten Kopierdurchlauf werden so eine Reihe von
  Speicherseiten noch zum Kopieren markiert worden sein. Es ist nicht
  klar ob diese Seiten vor oder nach der letzten Manipulation zum
  Zielsystem kopiert wurden (\autoref{fig:nomad_stage2}). Diese
  können nun in einem weiteren, kürzeren Kopierdurchlauf mit dem
  Zielsystem synchronisiert werden. Da dieser zweite Kopiervorgang
  schneller vonstatten geht, werden währenddessen weniger Seiten
  wieder vom Gastsystem beschrieben werden.
\item Nach mehreren Iterationen ist das Delta zwischen dem Gast und
  Zielsystem so weit geschrumpft, dass die verbleibende Differenz in
  wenigen Millisekunden kopiert werden kann. In diesem Moment wird der
  Gastprozess auf dem Ursprungssystem eingefroren, die restlichen
  Seiten kopiert, und auf dem Zielsystem gestartet
  (\autoref{fig:nomad_stage3}). Wenn im gleichen Moment auch das
  Routing angepasst wird, so kann die \ac{VM} direkt weiterarbeiten, ohne
  das Benutzer mehr als einen kurzen Latenzanstieg bemerken.
\end{enumerate}


Hansen arbeitet seit 2007 bei VMWare. VMWare bietet inzwischen ein
kommerzielles Produkt zur Live-Migration names \emph{vMotion}. Der
primäre Fokus dieses Produktes liegt, ähnlich wie beim NomadBIOS,
darin \acp{VM} innerhalb eines Netzwerkes mit gemeinsamem Speicher
wandern zu lassen, um größtmögliche Ressourcennutzung zu
erzielen. Dazu bietet VMWare vMotion die Möglichkeit,
sogenannte \emph{Resource Pools} zu definieren, in denen \acp{VM}
fortwährend automatisiert wandern können.

Um höhere Verfügbarkeit zu gewährleisten, weicht vMotion vom oben
etablierten Precopy-Schema ab~\cite{nelson2005fast}. Im letzten
Schritt wird die Ziel-\ac{VM} direkt gestartet, wenn die Ursprungs-\ac{VM}
eingefroren wird. Damit wird vermieden, dass beim Übertragen des
letzten Deltas eventuell offene TCP Verbindungen einen Timeout
erfahren. Die noch zum Kopieren ausstehenden Speicherseiten sind
markiert, und werden noch in die Ziel-\ac{VM} kopiert. Wenn
zwischenzeitlich darauf zugegriffen werden soll, werden sie als
nächstes über das Netzwerk zum Ziel übertragen. Das bedeutet, dass zu
Beginn der Laufzeit der Ziel-\ac{VM} noch eine Abhängigkeit zum
Ursprungssystem besteht, und die Verbindung zwischen beiden Systemen
noch kurze Zeit weiterbestehen muss.

Eine weitere Besonderheit an der VMWare Lösung ist, dass der VMWare
ESXi-Hypervisor über Para-Virtualisierung hinausgeht, und deshalb die
Migration von beliebigen Betriebsystemen zwischen beliebigen (von
VMWare unterstützten) Hardware-Architekturen
unterstützt~\cite{nelson2005fast}. Dafür wird Netzwerkhardware voll
virtualisiert, außerdem wird davon ausgegangen, dass Verbindungen zu
Speicher ausschließlich über \ac{SAN} oder \ac{NAS} bereitgestellt
werden, und dass alle \acp{VM} mit denselben \ac{SAN}-
bzw. \ac{NAS}-Servern verbunden sind.

\subsection{IBM}
IBM unterstützt die Linux Community in großem
Umfang~\cite{kroahhartman2007linux}. IBM hat zur Live-Migration von
Linux-\acp{VM} sowohl beim Xen-Projekt, als auch dem
Kernvirtualisierungsmodul \ac{KVM} im Rahmen des \reservoir-Projektes
wesentliche Beiträge geleistet~\cite{rochwerger2009reservoir}. Dabei
lag der Fokus der angestrebten Lösung explizit auf Interoperabilität
zwischen verschiedenen Anbietern und der Möglichkeit, Migration über
Distanzen hinweg zu ermöglichen. Beides zusammen soll die Kosten für
benötigte Infrastruktur (wie z.B. \ac{SAN}-/\ac{NAS}-Server) senken
und die Skalierbarkeit über die Grenzen eines einzelnen Cluster
Netzwerks hinweg erhöhen.

Die Lösung hierfür sind \acp{VEEM}. Ein \ac{VEEM} verteilt \acp{VM}
(in diesem Kontext \acp{VEE} genannt) über verfügbare Host-Systeme,
unter Berücksichtigung bestimmter Einschränkungen, dazu können
\zB auch gesetzliche Bedingungen oder \acp{SLA} zählen. Außerdem
können mehrere \acp{VEE} zu einer \ac{VEE}-Gruppe zusammengefasst
werden, um bestimmte Dienste nah beieinander zu halten und nur
gemeinsam zu verwalten. IBM hat hierbei speziell darauf geachtet,
offene Protokolle zu erstellen, um die Struktur der Lösung unabhängig
von den verwendeten Virtualisierungsplatformen (\zB Hypervisoren) zu
machen. Auf diese Weise wird ein Migrationspfad für Cloud Anbieter
geöffnet, der es ermöglicht, bestehende Infrastruktur inkrementell mit
den nötigen \acp{API} auszustatten und so interoperabel mit vielen
verschiedenen Cloud-Lösungen zu werden.

\subsection{HP}
HP bietet Virtualisierungsdienste auf Basis von Microsoft's
\emph{HyperV}. \emph{HyperV} unterstützt in Windows Server 2008 R2
bereits Live-Migration von virtuellen Maschinen, der Migrationsprozess
läuft dabei analog zum NomadBIOS ab. Eine wesentliche Limitierung des
HyperV ist, dass alle physikalischen Maschinen Zugriff auf gemeinsamen
Speicher (\ac{NAS}\,/\,\ac{SAN}) haben müssen, um \ac{VM}-Daten
auszutauschen~\cite{hp2010hyperV}. Das kann beim Umziehen zwischen
geografisch entfernten Orten oft nicht gewährleistet werden. HP hat
hierfür sein \ac{CLX}-Produkt erweitert. \ac{CLX} ist zunächst eine
Software zur Replikation von Daten über große Distanzen. Mit den
HyperV spezifischen Erweiterungen kann \ac{CLX} nun dazu genutzt
werden, den Zustand von \acp{VM} zwischen verschiedenen Rechenzentren
zu synchronisieren. Dabei ist \ac{CLX} voll in den
HyperV-Migrationsprozess integriert. Mit der \ac{CLX}-HyperV
Kombination kann sichergestellt werden, das Replikation über
Datencenter hinweg \ac{VM}-Migrationen unterstützt, und nicht
behindert, indem während der Migration dieser mehr Bandbreite
zugestanden wird, damit die finale Speicherseitensynchronisation
(während der die Ursprungs-\ac{VM} eingefroren ist) so schnell wie
möglich vonstatten geht. Wenn die Migration abgeschlossen ist, macht
\ac{CLX} den neuen physikalischen Standort der \ac{VM} automatisch zum
Master für die weitere Replikation.

Im März 2010 aktualisierte HP außerdem seine UNIX Distribution
\emph{HP-UX}. Teil dieses Updates war die Version 4.2 des hauseigenen
\emph{Integrity Virtual Machines} Hypervisors. Dieser enthält eine
\emph{Online Migration} Funktion, die ebenso mit \ac{CLX} zusammen
eingesetzt werden kann~\cite{hp2010integrity}.

\subsection{Oracle}

Oracle bietet in Solaris (vormals bei Sun) eine betriebssystemintegrierte
Form der Virtualisierung an: Solaris Zones. Im Gegensatz zu den vorher
vorgestellten Lösungen werden in den Zones keine vollständigen
Betriebsysteme ausgeführt, vielmehr gibt es eine
Betriebssysteminstanz, genannt \emph{global zone}, in der der
Betriebssystemkern läuft und mehrere \emph{non-global zones}, die
diesen Kern mitbenutzen. Den \emph{non-global zones} erscheint der
Kern jedoch als ihr "`eigener"'. Solaris Zones werden zumeist zur
Separierung von Applikationen genutzt und mit Resourcenmanagement
verbunden; die Kombination wird \emph{Solaris Container}
genannt~\cite{price2004solaris}.

Im Kontext von Live-Migration sind Solaris Zones (und damit auch
Container) jedoch ungeeignet. Es existieren keinerlei Vorrichtungen
dafür im Kern von Solaris, geschweige denn Werkzeuge. Auch
nicht-Live-Migrationen sind mit hohem Manuellen aufwand verbunden und
entsprechen eher dem Prozess "`Herunterfahren, exportieren, kopieren,
importieren, hochfahren"'~\cite{Kimchi:Solaris-Zones-m}. Die Migration
von Zones als Standard-Anwendungsfall ist in Solaris nicht vorgesehen.

Oracle \acfp{ldom} (jetzt VM Server for SPARC), die auf SPARC-Servern
der T-Serie und einigen anderen Systemen zur Verfügung stehen,
unterstützen seit der Version 1.2 aus dem Jahr 2009
Live-Migration~\cite{Laurent:Answering-a-cus}. Diese
hardware-gestützte Virtualisierungsvariante bietet physisch getrennte
Gast-Systeme (Domänen) und dabei die Rekonfigurierbarkeit dieser im
eingeschalteten Zustand. Zur Live-Migration von Domänen wird auf
beiden Seiten nicht nur die selbe Software (in dem Fall auch Firmware)
genutzt, auch die Hardware muss in Prozessortyp und -frequenz
übereinstimmen. Es ist auch nicht möglich, von Vornherein bestimmte
Auslastungsgrenzen zu bestimmen, ab denen eine Live-Migration
vorgenommen wird. Vielmehr wird bei Auslastungsändereungen direkt auf
dem Server reagiert, indem \zB Prozessoren dazu- oder abgeschaltet
werden oder der für die Domäne verfügbarer Speicher verändert
wird~\cite{2010:Oracle-VM-Serve}. Als Betriebssystem in den \acp{ldom}
sind praktisch nur Solaris (und -- dank Linux-Kern-Emulation in
bestimmten Solaris Zones -- ältere Linux) bekannt, theoretisch ist es
aber möglich, beliebige Betriebssysteme, die SPARC-Tx-Prozessoren
unterstützen, zu nutzen; so \zB natives
Linux~\cite{Chartre:Linux-with-LDom}.



\subsection{Vergleich}
Wir kehren nun zurück zu den zuvor beschriebenen Anwendungsfällen für
\ac{VM}-Live-Migration (\autoref{sec:livemigration}). Die Fähigkeiten
und Ziele der in \autoref{sec:live-migr-syst} beschriebenen Lösungen
machen sie unterschiedlich gut für die beschriebenen Zwecke einsetzbar
(\autoref{tab:anw-anb-matrix}). Im Folgenden werden wir die Systeme
gemäß ihrer Ziele und Beschreibungen kurz mit den entsprechenden
Beispielanwendungen in Beziehung setzen. Dieser Vergleich basiert zum
größten Teil auf den Informationen der Hersteller.

\begin{table*}[htbp]
  \caption{Anwendung-Anbieter-Matrix}
  \centering
  \normalsize
  \smallskip
  \begin{tabular}[h]{r | c@{\qquad}c@{\qquad}c@{\qquad}c}
    & VMWare & IBM & HP & Oracle \\
    Verlässlichkeit & \checked & \checked & \checked &  \\
    Interoperabilität & \checked & \checked & &  \\
    Inter-Cloud Verschiebungen & & \checked & (\checked) & \\
    Hardware Maintenance & \checked & (\checked) & \checked & \checked \\
    Adaptive Auslastung & \checked & \checked & \checked & (\checked) \\
    Erzwungene Verschiebungen & & (\checked) & (\checked) & \\
  \end{tabular}
  \smallskip
 \label{tab:anw-anb-matrix}
\end{table*}


\paragraph*{Verlässlichkeit}
Hardware-Fehler sind ein Problem das sich nicht in absehbarer Zeit
lösen lässt. Virtualisierung bietet allerdings die Möglichkeit,
angebotene Dienste von physikalischer Hardware zu entkoppeln. Mit
Live-Migration kann mit einigen Hardwarefehlern umgegangen werden,
ohne dass deshalb der oder die Dienste, die gerade auf der Hardware
laufen, abgeschaltet werden müssen.

Alle vorgestellten Systeme können mit diesem Szenario umgehen. Nach
den \acp{ldom} von Oracle hat VMWare mit vMotion out-of-the-box hier
wohl die engsten Begrenzungen, da \ac{SAN} bzw. \ac{NAS} Server
gemeinsam von Quell- und Zielsystemen erreichbar sein müssen. Das
heißt, dass der Ausfall eines gesamten Rechenzentrums nicht ohne
Mehraufwand kompensierbar ist, da hierfür Replikation des
Netzwerkspeichers getrennt gelöst werden muss. Hier bietet HP mit
\ac{CLX} und HyperV die integrierte Lösung, und macht sogar explizit
Werbung mit der Migration von Streaming-Services sogar bei
katastrophalen Fehlern in einem Rechenzentrum. Die von IBM erstellten
\acp{API} wurden ebenfalls explizit mit dem Gedanken erstellt, Migration
über große Entfernungen zu ermöglichen, und sind deshalb \emph{per se}
geeignet, um Dienste gegen weitreichende Hardware-Ausfälle
abzusichern.

\paragraph*{Interoperabilität}
Virtualisierungslösungen existieren unabhängig von der Cloud
(\autoref{sec:def-virtualisierung}) und es besteht Interesse, die
Abhängigkeit von einem bestimmten Hersteller so gering wie möglich zu
halten, indem bei den verwendeten Systemen auf Interoperabilität
geachtet wird. Live-Migration ist hier interessant, damit beim Wechsel
der Virtualisierungslösung nicht Dienste offline gehen müssen.

Hier sind die para-virtualisierenden Systeme (Xen, \ac{KVM})
eingeschränkter, da am Gast-System Anpassungen vorgenommen werden
müssen, die unter Umständen nicht im Betrieb zu ändern sind, weshalb man nicht
einfach ein System in ein anderes übertragen kann. Bei Solaris Zones
ist ein Gastsystem meist gar kein vollständiges System, sondern teilt
wesentliche Teile der POSIX Umgebung mit den anderen Gästen und dem
Host. Ein Extrahieren eines einzelnen Gastes ist gar nicht ohne
weiteres möglich. Bei den \acp{ldom} besteht wiederum das Problem,
dass die virtualisierten System zwar volle Betriebssysteme sind, die
Virtualisierung jedoch zu weiten Teilen aus der Firmware kommt und
damit unportabel ist. VMWare ESXi, als das vorgestellte System mit dem
höchsten Level der Virtualisierung, macht das Umwandeln in andere
Systeme derzeit am leichtesten, da Speicher im Netzwerk liegt und
weite Teile der Hardware virtualisiert sind, weshalb Gastssysteme hier
auch ohne Anpassungen laufen können. Aber auch hier ist eine
Live-Migration in ein anderes System unseres Wissens nicht möglich.

Auf Dauer ist das \reservoir-Projekt von IBM am ehesten geeignet, die
Beschränkungen beim Umstieg von einem System auf das andere
Aufzuheben, und tatsächlich Live-Migration zwischen den Systemen zu
bieten. Die Abstraktion von verwendeten Hypervisor-Technologien und
dem verwendeten Speicher wird es ermöglichen, zwischen zu den
beschriebenen Schnittstellen kompatiblen Systemen dynamisch zu
migrieren.

\paragraph*{Inter-Cloud Verschiebungen}
Ebenso wie die Abhängigkeit von einer Virtualisierungslösung, soll die
Bindung an einen Cloud-Anbieter möglichst schwach bleiben. Deshalb
sollte es möglich sein, Dienste auch zwischen Clouds zu verschieben.

Derzeit gibt es keinen einheitlichen Standard für Live-Migration und
die verwendeten Protokolle. Wie bereits angemerkt unterstützen mit
Ausnahme von HP und IBM die Systeme von sich aus gar keine
Verschiebung zwischen Netzwerken mit getrenntem Speicher. Weiterhin
sind die Schnittstellen der Hypervisoren nicht einheitlich,
was eine Migration zwischen Clouds mit verschiedenen
Virtualisierungstechnologien erschwert. Derzeit ist unter Umständen
möglich über Umwege wie selbst eingerichtete Replikation zwischen
Cloud-Anbietern mit denselben Technologien Live-Migration
durchzuführen. Wirklich möglich wird auch das allerdings erst, wenn
die \reservoir-Schnittstellen oder die vorgeschlagenen Cloud-Standards
von vielen Lösungen implementiert werden.

\paragraph*{Hardware-Maintenance}
Die Kosten, alte Hardware am Laufen zu halten, übersteigen nach einiger
Zeit immer die Anschaffungskosten für neue, bessere Hardware. Deshalb
ist es im Interesse eines Unternehmens, neue Hardware nutzen zu
können, ohne Verfügbarkeit einschränken zu müssen.

Hier punkten die integrierten Lösungen von HP, Oracle und VMWare, da
sie bereits die Abstraktion von Ressourcen ermöglichen, und Verwaltung
von Hardware als \emph{Resource Pools} vereinheitlichen. Dabei kann
neue Hardware hinzugefügt und alte entfernt werden, und das System
konfiguriert sich selbst neu und verteilt die Last automatisch auf die
neu verfügbare Hardware. Auch IBM kennt diese Abstraktion in Form von
\acp{VEE}, allerdings muss diese Lösung noch entsprechend weitreichend
implementiert werden. Auch bei der \ac{ldom}-basierten Lösung ist es
möglich, dass zwar Hardware eines Systems aufgestockt und ausgetauscht
werden kann, ohne Gast-\ac{VM}s herunterfahren zu müssen.

\paragraph*{Adaptive Auslastung}
Dienste, die sich auf bestimmte Zielgruppen, Länder oder Themengebiete
spezialisieren, sehen oft stark schwankende Nutzerzahlen, sei es, weil
bestimmte Ereignisse Zugriffe in die Höhe treiben, oder schlicht weil
die Zielgruppe generell im selben Land zu finden ist, und tendenziell
zu ähnlichen Zeiten den Dienst in Anspruch nimmt. Als Anbieter hat man
hier ein Interesse daran, nur die Menge an Hosting-Kosten zu zahlen,
die zu einem gegebenen Zeitpunkt benötigt wird. Dazu kann
Live-Migration beitragen, indem Dienste je nach Last auf schnellere
oder langsamere Systeme verschoben werden, und so mehr oder weniger
CPU-Zeit bekommen.

Hier sind alle vorgestellten Systeme einsetzbar, der Umfang der
Schwankungen sollte über die Auswahl entscheiden. Bei vMotion und
\ac{CLX} sind relativ weitreichende Änderungen der physikalischen
Hardware beim Migrieren möglich, was in bei einer hohen Varianz der
verwendeten Hardware nützlich ist. Bei den von IBM unterstützten
Lösungen Xen und \ac{KVM} müssen die unterliegenden Systeme relativ
ähnlich sein, was die Hardwareauswahl einschränkt. Bei
\ac{ldom}-basierten Lösungen, schließlich, muss die Hardware nahezu
identisch sein.


\paragraph*{Erzwungene Verschiebungen}
In manchem Anwendungsgebieten müssen Dienste an bestimmten
geografischen Orten ausgeführt werden, sei es aus Sicherheitsgründen
oder weil ein Gesetz es vorschreibt. Ein Anbieter von
Datenverarbeitungsdiensten möchte nicht jedes mal, wenn eine solche
Einschränkung in Kraft tritt, bestehende, und möglicherweise lang
laufende Jobs abbrechen, um sie an einem anderen Ort neu zu starten.

Die \ac{ldom}-Lösung fällt hier durch, da Migration über geografische
Entfernungen nicht angemessen möglich sind. Auch vMotion ist
ungeeignet, da nicht ohne weiteres zwischen entfernten Netzwerken
migriert werden kann. HP bietet hier zwar mit der \ac{CLX} eine
Lösung, allerdings steht zumindest zu vermuten, dass die reine
Ausführung an anderer Stelle nicht eventuellen Sicherheits- oder
Gesetzesrichtlinien entspricht, wenn es eine Replikation der Daten an
verschiedenen Orten gibt. Trotzdem scheint das derzeit die einzige
verfügbare Lösung zu sein. Einzig die von IBM vorgeschlagenen
\acp{VEEM} wären hier besser geeignet, da sie, laut Spezifikation, die
Möglichkeit bieten sollen, derlei Einschränkungen an den Ort der
\ac{VM}-Daten zu konfigurieren. Damit ließe sich eine allgemeine
Lösung auch automatisieren.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "FelgentreffPape_2010_Live-MigrationInVirtuellenUmgebungen"
%%% End:
