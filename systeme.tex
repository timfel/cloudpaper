\section{Live-Migration-Systeme}
\label{sec:live-migr-syst}
Im Folgenden wollen wir einige konkrete Implementierungen der oben
vorgestellten Techniken vorstellen, und deren Limitationen und
Einsatzbarkeit für die oben genannten Use-Cases betrachten.

\subsection{NomadBIOS und vMotion}
\label{sec:nomadbios--vmware}
Wie im Vortrag von Dr. Jacob Gorm Hansen beschrieben, war die erste
vollständige Implementierung zur Live-Migration einer kompletten VM
der NomadBIOS Hypervisor. Asger Jensen und Dr. Jacob Gorm Hansen
entwickelten diesen Hypervisor auf dem L4 Microkernel und
demonstrierten als erste erfolgreich Live-Migration von virtuellen
Maschinen über ein lokales Netzwerk. Die Motivation dieser Arbeit kam
aus dem Grid-Computing: Für jeden Job sollte eine VM gestartet werden,
die dann, mit Prioritäten ausgestattet, durch das Grid verschoben
werden konnte, auf der Suche nach freien Ressourcen. Daher rührt auch
der Name \emph{NomadBIOS}, ein "`nomadisches"' System.

Im Rahmen ihrer Master-Arbeit implementierten Jacob Gorm Hansen und
Asger Kahl Henriksen den Hypervisor zunächst auf dem L4
Kernel. Gastsysteme mussten angepasst werden, um auf diesem
teilvirtualisierten System als Userspace Prozess ausgeführt zu
werden. Im Umkehrschluss konnten viele der Prinzipien hinter
Prozessmigration verwendet werden, um Betriebssystemprozesse zu
verschieben. Außerdem verwendet das NomadBIOS \emph{Checkpointing},
den Akt, den Speicherzustand von Prozessen einzufrieren um sie später
fortsetzen zu können.

\subsubsection{Implementierungsdetails des NomadBIOS}
\label{sec:impl-des-nomadb}
Im NomadBIOS werden mehrere Userspace Module für den L4 Kernel
implementiert, die dazu dienen, Gastbetriebssysteme von der Hardware
zu isolieren~\cite{hansen2002nomadic}. Dazu gehören \emph{Paging}, für
die Isolation des Speichermanagements, \emph{Interrupt Multiplexing}
um Zugriff auf Hardware zu verweben und \emph{Address Space
  Translation}, um einfache Kommunikation zwischen dem Gast- und dem
Hostbetriebssystem zu ermöglichen. Letzteres wird genutzt, um
Unterstützung für den Migrationsvorgang im Gastsystem implementieren
zu können.

\begin{figure}[b]
  \centering
  \includegraphics[width=0.7\linewidth]{images/nomad_stage1}
  \caption{NomadBIOS: 1. Migrationsphase}
  \label{fig:nomad_stage1}
\end{figure}
Für die eigentliche Migration implementiert das NomadBIOS ein
\emph{Precopy} Scheme, wie es bereits aus der Prozessmigration bekannt
ist~\cite{hansen2002nomadic}.
\begin{enumerate}
\item In Abbildung~\ref{fig:nomad_stage1} ist der Startzustand der
  Migration dargestellt. Initial ist der Zustand allen Speichers auf
  dem Zielsystem unbekannt (helles Grau). Der gesamte Speicher vom
  Ursprungssystem muss zum Kopieren ausgewählt werden (rot). Der
  Speicher auf dem Ursprung, der dem Gastsystem gehört, wird zunächst
  für Schreibzugriffe gesperrt und der Kopiervorgang auf den neuen
  Host wird gestartet. Wenn währenddessen Speicher geschrieben wird,
  wird ein Page-fault ausgelöst, und das NomadBIOS kann die
  entsprechende Speicherseite erneut zum Kopieren markieren. Danach
  wird die Seite als für das Gastsystem beschreibbar markiert und
  geschrieben.
\item Nach einem ersten Kopierdurchlauf werden so eine Reihe von
  Speicherseiten noch zum Kopieren markiert worden sein. Es ist nicht
  klar ob diese Seiten vor oder nach der letzten Manipulation zum
  Zielsystem kopiert wurden (Abb.~\ref{fig:nomad_stage2}). Diese
  können nun in einem weiteren, kürzeren Kopierdurchlauf mit dem
  Zielsystem synchronisiert werden. Da dieser zweite Kopiervorgang
  schneller vonstatten geht, werden währenddessen weniger Seiten
  wieder vom Gastsystem beschrieben werden.
\item Nach mehreren Iterationen ist das Delta zwischen dem Gast und
  Zielsystem so weit geschrumpft, dass die verbleibende Differenz in
  wenigen Millisekunden kopiert werden kann. In diesem Moment wird der
  Gastprozess auf dem Ursprungssystem eingefroren, die restlichen
  Seiten kopiert, und auf dem Zielsystem gestartet
  (Abb.~\ref{fig:nomad_stage3}). Wenn im gleichen Moment auch das
  Routing angepasst wird, so kann die VM direkt weiterarbeiten, ohne
  das Benutzer mehr als einen kurzen Latenzanstieg bemerken.
\end{enumerate}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\linewidth]{images/nomad_stage2}
  \caption{NomadBIOS: 2. Migrationsphase}
  \label{fig:nomad_stage2}
\end{figure}
\begin{figure}[b]
  \centering
  \includegraphics[width=0.7\linewidth]{images/nomad_stage3}
  \caption{NomadBIOS: 3. Migrationsphase}
  \label{fig:nomad_stage3}
\end{figure}

Dr. Hansen arbeitet seit 2007 bei VMWare. VMWare bietet inzwischen ein
kommerzielles Produkt zur Live-Migration names \emph{vMotion}. Der
primäre Fokus dieses Produktes liegt, ähnlich wie beim NomadBIOS,
darin VMs innerhalb eines Netzwerkes mit gemeinsamem Speicher wandern
zu lassen, um größtmögliche Ressourcennutzung zu erzielen. Dazu bietet
VMWare vMotion die Möglichkeit, sog. \emph{Resource Pools} zu
definieren, in denen VMs fortwährend automatisiert wandern
können. 

Um höhere Verfügbarkeit zu gewährleisten, weicht vMotion vom oben
etablierten Precopy-Schema ab~\cite{nelson2005fast}. Im letzten
Schritt wird die Ziel-VM direkt gestartet, wenn die Ursprungs-VM
eingefroren wird. Damit wird vermieden, dass beim Übertragen des
letzten Deltas eventuell offene TCP Verbindungen einen Timeout
erfahren. Die noch zum Kopieren ausstehenden Speicherseiten sind
markiert, und werden noch in die Ziel-VM kopiert. Wenn
zwischenzeitlich darauf zugegriffen werden soll, werden sie als
nächstes über das Netzwerk zum Ziel übertragen. Das bedeutet, dass zu
Beginn der Laufzeit der Ziel-VM noch eine Abhängigkeit zum
Ursprungssystem besteht, und die Verbindung zwischen beiden Systemen
noch kurze Zeit weiterbestehen muss.

Eine weitere Besonderheit an der VMWare Lösung ist, dass der VMWare
ESX Hypervisor über Para-Virtualisierung hinausgeht, und deshalb die
Migration von beliebigen Betriebsystemen zwischen beliebigen (von
VMWare unterstützten) Hardware Architekturen
unterstützt~\cite{nelson2005fast}. Dafür wird Netzwerkhardware voll
virtualisiert, außerdem wird davon ausgegangen, dass Verbindungen zu
Speicher ausschließlich über \emph{Storage Area Networks} (SAN) oder
\emph{Network Attached Storage} (NAS) bereitgestellt werden, und dass
alle VMs mit denselben SAN- bzw. NAS-Servern verbunden sind.

\subsection{IBM}
IBM unterstützt die Linux Community in großem
Umfang~\cite{kroahhartman2007linux}. IBM hat zur Live-Migration von
Linux VMs hat IBM sowohl beim Xen Projekt, als auch dem Kernel
Virtualisierungsmodul KVM (\emph{Kernel Virtual Machine}) wesentliche
Beiträge geleistet~\cite{rochwerger2009reservoir}. Dabei lag der Fokus
der angestrebten Lösung explizit auf Interoperabilität zwischen
verschiedenen Providern und der Möglichkeit, Migration über Distanzen
hinweg zu ermöglichen. Beides zusammen soll die Kosten für benötigte
Infrastruktur (wie z.B. SAN-/NAS-Server) senken und die Skalierbarkeit
über die Grenzen eines einzelnen Cluster Netzwerks hinweg erhöhen. 

\subsection{HP}
HP bietet Virtualisierungsdienste auf Basis von Microsoft's
\emph{HyperV}. \emph{HyperV} unterstützt in Windows Server 2008 R2
bereits Live-Migration von virtuellen Maschinen, der Migrationsprozess
läuft dabei analog zum NomadBIOS ab. Eine wesentliche Limitierung des
HyperV ist, dass alle physikalischen Maschinen Zugriff auf gemeinsamen
Speicher, z.B. Network-Attached-Storage oder ein Netzwerkdateisystem,
haben müssen, um VM Daten auszutauschen~\cite{hp2010hyperV}. Das kann
beim Umziehen zwischen geografisch entfernten Orten oft nicht
gewährleistet werden. HP hat hierfür sein \emph{Cluster Extension}
(CLX) Produkt erweitert. CLX ist zunächst eine Software zur
Replikation von Daten über große Distanzen. Mit den HyperV
spezifischen Erweiterungen kann CLX nun dazu genutzt werden, den
Zustand von VMs zwischen verschiedenen Rechenzentren zu
synchronisieren. Dabei ist CLX voll in den HyperV Live-Migration
Prozess integriert. Mit der CLX---HyperV Kombination kann
sichergestellt werden, das Replikation über Datencenter hinweg
VM-Migrationen unterstützt, und nicht behindert, indem während der
Migration dieser mehr Bandbreite zugestanden wird, damit die finale
Speicherseitensynchronisation (während der die Ursprungs-VM
eingefroren ist) so schnell wie möglich vonstatten geht. Wenn die
Migration abgeschlossen ist, macht CLX den neuen physikalischen
Standort der VM automatisch zum Master für die weitere Replikation.

Im März 2010 aktualisierte außerdem seine UNIX Distribution
\emph{HP-UX}. Teil dieses Updates war die Version 4.2 des hauseigenen
\emph{Integrity Virtual Machines} Hypervisors. Dieser enthält eine
\emph{Online Migration} Funktion, die ebenso mit CLX zusammen
eingesetzt werden kann~\cite{hp2010integrity}.

\subsection{Sun/Solaris}
Solaris Container

\subsection{Vergleich}
Wir kehren nun zurück zu den zuvor beschriebenen Anwendungsfällen für
VM-Live-Migration~\ref{sec:livemigration}. Die Fähigkeiten und Ziele
der in Abschnitt~\ref{sec:live-migr-syst} beschriebenen Lösungen
machen sie unterschiedlich gut für die beschriebenen Zwecke
einsetzbar (Tab.~\ref{tab:anw-anb-matrix}).
Vergleich der Eignung als Lösung für die oben genannten Probleme
zwischen dem von Dr. Jacob Gorm Hansen vorgestellen System und den Alternativen.

\begin{table}[tb]
  \centering
  \begin{tabular}[h]{c|c c c c}
    & VMWare & IBM & HP & Solaris \\
    Verlässlichkeit & & \checked & & \\
    Interoperabilität & \checked & \checked & & \checked \\
    Inter-Cloud Verschiebungen & & \checked & & \\
    Hardware Maintenance & \checked & \checked & \checked & \\
    Adaptive Auslastung & \checked & & & \checked \\
    Erzwungene Verschiebungen & \checked & \checked & \checked & \\
  \end{tabular}
  \caption{Anwendung --- Anbieter Matrix}
  \label{tab:anw-anb-matrix}
\end{table}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "FelgentreffPape_2010_Live-MigrationInVirtuellenUmgebungen"
%%% End: 
